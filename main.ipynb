{
 "cells": [
  {
   "cell_type": "code",
   "id": "c92d1a7dc8826403",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-26T19:19:03.996972Z",
     "start_time": "2025-01-26T19:19:02.013979Z"
    }
   },
   "source": [
    "import sys\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "from motion_detector.motion_detector import MotionDetector\n",
    "from people_detector.people_detector import PeopleDetector\n",
    "from face_recognizer.face_recognizer import FaceRecognizer\n",
    "from utils.bbox_utils import crop_bboxes\n",
    "from utils.view import view\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import utils.logger as logger\n",
    "from logging import DEBUG, INFO, CRITICAL\n",
    "\n",
    "logger.init_logger(CRITICAL)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lorenzoantonelli/Desktop/ComputerVision/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "e2447958e18206ed",
   "metadata": {},
   "source": [
    "## PROCESS VIDEO\n",
    "*Here happens the magick*\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "8a3902c9f9ae4861",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-26T19:19:04.062601Z",
     "start_time": "2025-01-26T19:19:04.058528Z"
    }
   },
   "source": [
    "def process_video(video_path: str, motion_detector: MotionDetector, people_detector: PeopleDetector,\n",
    "                  face_recognizer: FaceRecognizer, scale: float = 0.5,\n",
    "                  early_stop=None, starts_from=0):\n",
    "    \"\"\"\n",
    "    Processes the input video.\n",
    "    Parameters:\n",
    "        video_path: Path to the video file.\n",
    "        scale: Scaling factor for resizing frames.\n",
    "        overlap_threshold: Threshold for merging overlapping detections using IoU.\n",
    "        area_threshold: Minimum area for detected bounding boxes.\n",
    "        people_detector: YOLO model to detect people.\n",
    "        early_stop: stop after n frames\n",
    "    \"\"\"\n",
    "    capture = cv.VideoCapture(video_path)\n",
    "    iterations = 0\n",
    "    while early_stop is None or iterations < early_stop:\n",
    "        iterations += 1\n",
    "        ret, frame = capture.read()\n",
    "        if iterations <= starts_from:\n",
    "            print(\"skipping frame {}\".format(iterations))\n",
    "            continue\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # frame = cv.resize(frame, None, fx=scale, fy=scale, interpolation=cv.INTER_LINEAR)\n",
    "        frame = cv.resize(frame, None, fx=scale, fy=scale, interpolation=cv.INTER_AREA)\n",
    "\n",
    "        # detections, merged_detections, frame = mog2_movement_detection(frame, background_subtractor=back_sub, area_threshold=area_threshold,\n",
    "        #                         overlap_threshold=overlap_threshold, draw=False)\n",
    "\n",
    "        detections, merged_detections, frame = motion_detector(frame, draw=False)\n",
    "\n",
    "        if not merged_detections:\n",
    "            continue\n",
    "\n",
    "        # drop area colum\n",
    "        # cropped_frames = crop_bboxes(frame, bboxes=[md[:-1] for md in merged_detections])\n",
    "        merged_detections = np.array(merged_detections)\n",
    "        cropped_frames = crop_bboxes(frame, bboxes=merged_detections[:, :-1])  # a-la numpy\n",
    "\n",
    "        # detect people on cropped frames\n",
    "\n",
    "        # probs, bboxes, result = people_detector.detect(frame)\n",
    "        # annotated_frame = result.plot()\n",
    "        # view(annotated_frame)\n",
    "\n",
    "        detections = people_detector.detect_on_frames(cropped_frames)\n",
    "        for detection in detections:\n",
    "            probs, _, result = detection\n",
    "            if len(probs) == 0:\n",
    "                continue\n",
    "            detected_person_image = result.orig_img\n",
    "\n",
    "            # Face detection + recognition\n",
    "\n",
    "            # Skip too small images\n",
    "            if detected_person_image.shape[0] < face_recognizer.min_face_size or detected_person_image.shape[\n",
    "                1] < face_recognizer.min_face_size:\n",
    "                continue\n",
    "\n",
    "            faces = face_recognizer.recognize_faces(detected_person_image)\n",
    "            if len(faces) > 0:\n",
    "                print(f\"Frame {iterations}: {len(faces)} faces detected.\")\n",
    "\n",
    "                for detected_face in faces:\n",
    "                    if detected_face['label'] is not None:\n",
    "                        print(f\"Detected face: {detected_face['label']} with confidence {detected_face['confidence']}\")\n",
    "                    else:\n",
    "                        print(f\"Detected unrecognized face! ðŸ˜­\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Stopped after {iterations} frames due early stopping condition.\")\n",
    "\n",
    "    capture.release()\n",
    "    cv.destroyAllWindows()\n",
    "\n",
    "    if sys.platform == 'darwin':\n",
    "        for _ in range(4):\n",
    "            cv.waitKey(1)\n",
    "\n",
    "    cv.destroyAllWindows()\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "b8d04d81258a7f91",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "id": "c51cdddc4972c528",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-26T19:19:04.297347Z",
     "start_time": "2025-01-26T19:19:04.066104Z"
    }
   },
   "source": [
    "yolosize = 'n'\n",
    "yolo11 = PeopleDetector(f\"yolo11{yolosize}.pt\", verbose=False, )\n",
    "yolo11.to('cpu')\n",
    "\n",
    "overlap_threshold = 0.0005\n",
    "area_threshold = 700\n",
    "motion_detector = MotionDetector(area_threshold=area_threshold, overlap_threshold=overlap_threshold)\n",
    "\n",
    "face_recognizer = FaceRecognizer(threshold=0.5)\n",
    "face_5 = Image.open('demo_images/face_5.jpg')\n",
    "face_recognizer.enroll_face(face_5, 'Michael Scott')\n",
    "\n",
    "# probs, bboxes, _ = yolo11.detect(\"The-Office-HD-Background.jpg\")\n",
    "# print('confidence scores', probs)\n",
    "# print('bboxes', bboxes)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "3946034d83bc6843",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-26T19:20:12.349249Z",
     "start_time": "2025-01-26T19:19:04.304056Z"
    }
   },
   "source": [
    "video_path = 'datasets/new_video.mp4'\n",
    "process_video(video_path, motion_detector, yolo11, face_recognizer=face_recognizer,\n",
    "              scale=0.5)  # , starts_from=3000, early_stop=3500)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 70: 1 faces detected.\n",
      "Detected unrecognized face! ðŸ˜­\n",
      "Frame 73: 1 faces detected.\n",
      "Detected unrecognized face! ðŸ˜­\n",
      "Frame 74: 1 faces detected.\n",
      "Detected unrecognized face! ðŸ˜­\n",
      "Frame 75: 1 faces detected.\n",
      "Detected unrecognized face! ðŸ˜­\n",
      "Frame 76: 1 faces detected.\n",
      "Detected unrecognized face! ðŸ˜­\n",
      "Frame 77: 1 faces detected.\n",
      "Detected unrecognized face! ðŸ˜­\n",
      "Frame 78: 1 faces detected.\n",
      "Detected unrecognized face! ðŸ˜­\n",
      "Frame 79: 1 faces detected.\n",
      "Detected unrecognized face! ðŸ˜­\n",
      "Frame 80: 1 faces detected.\n",
      "Detected unrecognized face! ðŸ˜­\n",
      "Frame 81: 1 faces detected.\n",
      "Detected unrecognized face! ðŸ˜­\n",
      "Frame 82: 1 faces detected.\n",
      "Detected face: Marini with confidence 0.5553990006446838\n",
      "Frame 83: 1 faces detected.\n",
      "Detected unrecognized face! ðŸ˜­\n",
      "Frame 84: 1 faces detected.\n",
      "Detected face: Marini with confidence 0.5688936114311218\n",
      "Frame 85: 1 faces detected.\n",
      "Detected unrecognized face! ðŸ˜­\n",
      "Frame 86: 1 faces detected.\n",
      "Detected unrecognized face! ðŸ˜­\n",
      "Frame 87: 1 faces detected.\n",
      "Detected unrecognized face! ðŸ˜­\n",
      "Frame 88: 1 faces detected.\n",
      "Detected unrecognized face! ðŸ˜­\n",
      "Frame 89: 1 faces detected.\n",
      "Detected unrecognized face! ðŸ˜­\n",
      "Frame 90: 1 faces detected.\n",
      "Detected face: Marini with confidence 0.5029153227806091\n",
      "Frame 91: 1 faces detected.\n",
      "Detected unrecognized face! ðŸ˜­\n",
      "Frame 92: 1 faces detected.\n",
      "Detected unrecognized face! ðŸ˜­\n",
      "Frame 93: 1 faces detected.\n",
      "Detected unrecognized face! ðŸ˜­\n",
      "Frame 94: 1 faces detected.\n",
      "Detected unrecognized face! ðŸ˜­\n",
      "Frame 95: 1 faces detected.\n",
      "Detected unrecognized face! ðŸ˜­\n",
      "Frame 96: 1 faces detected.\n",
      "Detected unrecognized face! ðŸ˜­\n",
      "Frame 97: 1 faces detected.\n",
      "Detected face: Marini with confidence 0.530585527420044\n",
      "Frame 98: 1 faces detected.\n",
      "Detected unrecognized face! ðŸ˜­\n",
      "Frame 99: 1 faces detected.\n",
      "Detected unrecognized face! ðŸ˜­\n",
      "Frame 100: 1 faces detected.\n",
      "Detected unrecognized face! ðŸ˜­\n",
      "Frame 101: 1 faces detected.\n",
      "Detected unrecognized face! ðŸ˜­\n",
      "Frame 102: 1 faces detected.\n",
      "Detected unrecognized face! ðŸ˜­\n",
      "Frame 103: 1 faces detected.\n",
      "Detected unrecognized face! ðŸ˜­\n",
      "Frame 104: 1 faces detected.\n",
      "Detected unrecognized face! ðŸ˜­\n",
      "Frame 107: 1 faces detected.\n",
      "Detected unrecognized face! ðŸ˜­\n",
      "Frame 192: 1 faces detected.\n",
      "Detected unrecognized face! ðŸ˜­\n",
      "Frame 193: 1 faces detected.\n",
      "Detected unrecognized face! ðŸ˜­\n",
      "Frame 194: 1 faces detected.\n",
      "Detected unrecognized face! ðŸ˜­\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m video_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdatasets/new_video.mp4\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mprocess_video\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvideo_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmotion_detector\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43myolo11\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mface_recognizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mface_recognizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m              \u001B[49m\u001B[43mscale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.5\u001B[39;49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# , starts_from=3000, early_stop=3500)\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[2], line 18\u001B[0m, in \u001B[0;36mprocess_video\u001B[0;34m(video_path, motion_detector, people_detector, face_recognizer, scale, early_stop, starts_from)\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m early_stop \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m iterations \u001B[38;5;241m<\u001B[39m early_stop:\n\u001B[1;32m     17\u001B[0m     iterations \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m---> 18\u001B[0m     ret, frame \u001B[38;5;241m=\u001B[39m \u001B[43mcapture\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     19\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m iterations \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m starts_from:\n\u001B[1;32m     20\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mskipping frame \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(iterations))\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "e027d6955f3367c9",
   "metadata": {},
   "source": [
    "# People Detection TEST\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1de7d0bb118f7b1",
   "metadata": {},
   "source": [
    "Define the base functions for people detection. <br/>\n",
    "It Takes as input an array of frames containing the motion detected by MOG2 above"
   ]
  },
  {
   "cell_type": "code",
   "id": "91ab0819e0ce2630",
   "metadata": {},
   "source": [
    "# imports"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4b39a6e73355cefc",
   "metadata": {},
   "source": [
    "# Load a model\n",
    "model = PeopleDetector(\"./yolo11x.pt\")  # load an official model, or use local path"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7ca67ec510bc568a",
   "metadata": {},
   "source": [
    "def test_yolo11(video_path: str, scale: float = 0.5):\n",
    "    # Open the video file\n",
    "    cap = cv.VideoCapture(video_path)\n",
    "\n",
    "    # Loop through the video frames\n",
    "    while cap.isOpened():\n",
    "        # Read a frame from the video\n",
    "        success, frame = cap.read()\n",
    "\n",
    "        if success:\n",
    "            # Run YOLO inference on the frame\n",
    "            # frame = cv.resize(frame, None, fx=0.5, fy=0.5, interpolation=cv.INTER_LINEAR)\n",
    "            results = model(frame)\n",
    "\n",
    "            # Visualize the results on the frame\n",
    "            annotated_frame = results[0].plot()\n",
    "            annotated_frame = cv.resize(annotated_frame, None, fx=scale, fy=scale, interpolation=cv.INTER_LINEAR)\n",
    "            # Display the annotated frame\n",
    "            cv.imshow(\"YOLO Inference\", annotated_frame)\n",
    "\n",
    "            # Break the loop if 'q' is pressed\n",
    "            if cv.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "                break\n",
    "        else:\n",
    "            # Break the loop if the end of the video is reached\n",
    "            break\n",
    "\n",
    "    # Release the video capture object and close the display window\n",
    "    cap.release()\n",
    "    cv.destroyAllWindows()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a311a48f20a0b0cb",
   "metadata": {},
   "source": [
    "## Main test YOLO11"
   ]
  },
  {
   "cell_type": "code",
   "id": "10ce9b273702bcbf",
   "metadata": {},
   "source": [
    "test_yolo11(\"datasets/SamsungGear360.mp4\", scale=0.25)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
