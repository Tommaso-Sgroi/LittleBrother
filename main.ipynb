{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92d1a7dc8826403",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "from motion_detector.motion_detector import MotionDetector\n",
    "from people_detector.people_detector import PeopleDetector\n",
    "from face_recognizer.face_recognizer import FaceRecognizer\n",
    "from utils.bbox_utils import crop_bboxes\n",
    "from utils.view import view\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import utils.logger as logger\n",
    "from logging import DEBUG, INFO, CRITICAL\n",
    "\n",
    "logger.init_logger(CRITICAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2447958e18206ed",
   "metadata": {},
   "source": [
    "## PROCESS VIDEO\n",
    "*Here happens the magick*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3902c9f9ae4861",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(video_path: str, motion_detector: MotionDetector, people_detector: PeopleDetector,\n",
    "                  face_recognizer: FaceRecognizer, scale: float = 0.5,\n",
    "                  early_stop=None, starts_from=0):\n",
    "    \"\"\"\n",
    "    Processes the input video.\n",
    "    Parameters:\n",
    "        video_path: Path to the video file.\n",
    "        scale: Scaling factor for resizing frames.\n",
    "        overlap_threshold: Threshold for merging overlapping detections using IoU.\n",
    "        area_threshold: Minimum area for detected bounding boxes.\n",
    "        people_detector: YOLO model to detect people.\n",
    "        early_stop: stop after n frames\n",
    "    \"\"\"\n",
    "    capture = cv.VideoCapture(video_path)\n",
    "    iterations = 0\n",
    "    while early_stop is None or iterations < early_stop:\n",
    "        iterations += 1\n",
    "        ret, frame = capture.read()\n",
    "        if iterations <= starts_from:\n",
    "            print(\"skipping frame {}\".format(iterations))\n",
    "            continue\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # frame = cv.resize(frame, None, fx=scale, fy=scale, interpolation=cv.INTER_LINEAR)\n",
    "        frame = cv.resize(frame, None, fx=scale, fy=scale, interpolation=cv.INTER_AREA)\n",
    "\n",
    "        # detections, merged_detections, frame = mog2_movement_detection(frame, background_subtractor=back_sub, area_threshold=area_threshold,\n",
    "        #                         overlap_threshold=overlap_threshold, draw=False)\n",
    "\n",
    "        detections, merged_detections, frame = motion_detector(frame, draw=False)\n",
    "\n",
    "        if not merged_detections:\n",
    "            continue\n",
    "\n",
    "        # drop area colum\n",
    "        # cropped_frames = crop_bboxes(frame, bboxes=[md[:-1] for md in merged_detections])\n",
    "        merged_detections = np.array(merged_detections)\n",
    "        cropped_frames = crop_bboxes(frame, bboxes=merged_detections[:, :-1])  # a-la numpy\n",
    "\n",
    "        # detect people on cropped frames\n",
    "\n",
    "        # probs, bboxes, result = people_detector.detect(frame)\n",
    "        # annotated_frame = result.plot()\n",
    "        # view(annotated_frame)\n",
    "\n",
    "        detections = people_detector.detect_on_frames(cropped_frames)\n",
    "        for detection in detections:\n",
    "            probs, _, result = detection\n",
    "            if len(probs) == 0:\n",
    "                continue\n",
    "            detected_person_image = result.orig_img\n",
    "\n",
    "            # Face detection + recognition\n",
    "\n",
    "            # Skip too small images\n",
    "            if detected_person_image.shape[0] < face_recognizer.min_face_size or detected_person_image.shape[\n",
    "                1] < face_recognizer.min_face_size:\n",
    "                continue\n",
    "\n",
    "            faces = face_recognizer.recognize_faces(detected_person_image)\n",
    "            if len(faces) > 0:\n",
    "                print(f\"Frame {iterations}: {len(faces)} faces detected.\")\n",
    "\n",
    "                for detected_face in faces:\n",
    "                    if detected_face['label'] is not None:\n",
    "                        print(f\"Detected face: {detected_face['label']} with confidence {detected_face['confidence']}\")\n",
    "                    else:\n",
    "                        print(f\"Detected unrecognized face! ðŸ˜­\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Stopped after {iterations} frames due early stopping condition.\")\n",
    "\n",
    "    capture.release()\n",
    "    cv.destroyAllWindows()\n",
    "\n",
    "    if sys.platform == 'darwin':\n",
    "        for _ in range(4):\n",
    "            cv.waitKey(1)\n",
    "\n",
    "    cv.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d04d81258a7f91",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51cdddc4972c528",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolosize = 'n'\n",
    "yolo11 = PeopleDetector(f\"yolo11{yolosize}.pt\", verbose=False, )\n",
    "yolo11.to('cpu')\n",
    "\n",
    "overlap_threshold = 0.0005\n",
    "area_threshold = 700\n",
    "motion_detector = MotionDetector(area_threshold=area_threshold, overlap_threshold=overlap_threshold)\n",
    "\n",
    "face_recognizer = FaceRecognizer(threshold=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3946034d83bc6843",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = 'datasets/SamsungGear360.mp4'\n",
    "process_video(video_path, motion_detector, yolo11, face_recognizer=face_recognizer,\n",
    "              scale=0.5)  # , starts_from=3000, early_stop=3500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e027d6955f3367c9",
   "metadata": {},
   "source": [
    "# People Detection TEST\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1de7d0bb118f7b1",
   "metadata": {},
   "source": [
    "Define the base functions for people detection. <br/>\n",
    "It Takes as input an array of frames containing the motion detected by MOG2 above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ab0819e0ce2630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b39a6e73355cefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a model\n",
    "model = PeopleDetector(\"./yolo11x.pt\")  # load an official model, or use local path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca67ec510bc568a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_yolo11(video_path: str, scale: float = 0.5):\n",
    "    # Open the video file\n",
    "    cap = cv.VideoCapture(video_path)\n",
    "\n",
    "    # Loop through the video frames\n",
    "    while cap.isOpened():\n",
    "        # Read a frame from the video\n",
    "        success, frame = cap.read()\n",
    "\n",
    "        if success:\n",
    "            # Run YOLO inference on the frame\n",
    "            # frame = cv.resize(frame, None, fx=0.5, fy=0.5, interpolation=cv.INTER_LINEAR)\n",
    "            results = model(frame)\n",
    "\n",
    "            # Visualize the results on the frame\n",
    "            annotated_frame = results[0].plot()\n",
    "            annotated_frame = cv.resize(annotated_frame, None, fx=scale, fy=scale, interpolation=cv.INTER_LINEAR)\n",
    "            # Display the annotated frame\n",
    "            cv.imshow(\"YOLO Inference\", annotated_frame)\n",
    "\n",
    "            # Break the loop if 'q' is pressed\n",
    "            if cv.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "                break\n",
    "        else:\n",
    "            # Break the loop if the end of the video is reached\n",
    "            break\n",
    "\n",
    "    # Release the video capture object and close the display window\n",
    "    cap.release()\n",
    "    cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a311a48f20a0b0cb",
   "metadata": {},
   "source": [
    "## Main test YOLO11"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
