{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-19T21:37:04.646499Z",
     "start_time": "2025-01-19T21:37:04.644065Z"
    }
   },
   "source": [
    "import sys\n",
    "from typing import Any\n",
    "\n",
    "from torch.xpu import device\n",
    "from ultralytics import YOLO\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 97
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Utilities functions",
   "id": "958cf11c42a5231c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T21:37:04.692074Z",
     "start_time": "2025-01-19T21:37:04.689539Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def view(frame, *, scale=0.5):\n",
    "    \"\"\"\n",
    "    :param frame: frame to draw on\n",
    "    :param scale: scale factor to scale the frame by\n",
    "    :return: stop the drawing of the frame\n",
    "    \"\"\"\n",
    "    # Resize frame to    a normal view\n",
    "    frame = cv.resize(frame, None, fx=scale, fy=scale, interpolation=cv.INTER_LINEAR)\n",
    "    cv.imshow('Frame', frame)\n",
    "    key = cv.waitKey(1)\n",
    "    if key in [27, ord('q'), ord('Q')]:\n",
    "        return False\n",
    "    return True"
   ],
   "id": "da8ad7ab85809bca",
   "outputs": [],
   "execution_count": 98
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T21:37:04.737326Z",
     "start_time": "2025-01-19T21:37:04.734333Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# utility crop on BBoxes\n",
    "def crop_bbox(frame: np.array, bbox: tuple[int, int, int, int]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Crops the frame based on the bounding box.\n",
    "    :param frame: frame to crop.\n",
    "    :param bbox: bbox to crop [x1, y1, x2, y2], NO AREA IS NEEDED.\n",
    "    :return: the new frame cropped.\n",
    "\n",
    "    bbox example: [1947,  475, 1954,  698, 1561]\n",
    "    \"\"\"\n",
    "    # calculate width and height\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    w = x2 - x1\n",
    "    h = y2 - y1\n",
    "\n",
    "    cropped_frame = frame[y1:y1+h, x1:x1+w]\n",
    "    return cropped_frame\n",
    "\n",
    "def crop_bboxes(frame, bboxes: list[tuple[int, int, int, int]]) -> list[np.ndarray]:\n",
    "    # cropped = []\n",
    "    # for bbox in bboxes:\n",
    "    #     cropped_frame = crop_bbox(frame, bbox)\n",
    "    #     cropped.append(cropped_frame)\n",
    "    return [\n",
    "        crop_bbox(frame, bbox) for bbox in bboxes\n",
    "    ]"
   ],
   "id": "e76dcf147ea20aed",
   "outputs": [],
   "execution_count": 99
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T21:37:04.783318Z",
     "start_time": "2025-01-19T21:37:04.779240Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def merge_overlapping_detections(detections: list, overlap_threshold: float = 0.3) -> list:\n",
    "    \"\"\"\n",
    "    Merges overlapping bounding boxes based on Intersection over Union (IoU).\n",
    "\n",
    "    Parameters:\n",
    "        detections: List of bounding boxes to merge.\n",
    "        overlap_threshold: IoU threshold for merging boxes. Boxes with IoU >= threshold are merged.\n",
    "\n",
    "    Returns:\n",
    "        list: List of merged bounding boxes.\n",
    "    \"\"\"\n",
    "    if not detections:\n",
    "        return []\n",
    "\n",
    "    boxes = np.array(detections)\n",
    "    x1, y1 = boxes[:, 0], boxes[:, 1]\n",
    "    x2, y2 = boxes[:, 2], boxes[:, 3]\n",
    "    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    '''\n",
    "    [     x      y    w      h   Area\n",
    "        [ 1842   647  1926   771 10416]\n",
    "        [ 1918   512  1947   575  1827]\n",
    "        [ 1855   467  1912   635  9576]\n",
    "     ]\n",
    "    ------------------------\n",
    "    [1842 1918 1855] all x1\n",
    "    ------------------------\n",
    "    [647 512 467] all y1\n",
    "    ------------------------\n",
    "    [1926 1947 1912] all x2\n",
    "    ------------------------\n",
    "    [771 575 635] all y2\n",
    "    ------------------------\n",
    "    [10625  1920  9802] all Area between them\n",
    "    '''\n",
    "\n",
    "    # Sort boxes by their area in descending order\n",
    "    order = areas.argsort()[::-1]  # a[start:end:step]\n",
    "    merged_boxes = []\n",
    "\n",
    "    while len(order) > 0:\n",
    "        i = order[0]\n",
    "        merged_boxes.append(boxes[i])\n",
    "\n",
    "        xx1 = np.maximum(x1[i], x1[order[1:]])\n",
    "        yy1 = np.maximum(y1[i], y1[order[1:]])\n",
    "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
    "        yy2 = np.minimum(y2[i], y2[order[1:]])\n",
    "\n",
    "        w = np.maximum(0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0, yy2 - yy1 + 1)\n",
    "\n",
    "        inter = w * h\n",
    "        union = areas[i] + areas[order[1:]] - inter\n",
    "        iou = inter / union\n",
    "\n",
    "        # Keep boxes with IoU below the threshold\n",
    "        remain_indices = np.where(iou < overlap_threshold)[0] + 1\n",
    "        order = order[remain_indices]\n",
    "\n",
    "    return merged_boxes"
   ],
   "id": "65e6db3652ee10e0",
   "outputs": [],
   "execution_count": 100
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## MOG2 Movement Detection",
   "id": "d9cf0bd925134c3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T21:37:04.828278Z",
     "start_time": "2025-01-19T21:37:04.825350Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def detect_moving_objects(frame: np.ndarray, background_subtractor: cv.BackgroundSubtractor,\n",
    "                          area_threshold: int = 100) -> tuple[Any, Any]:\n",
    "    \"\"\"\n",
    "    Detects moving objects using a background subtractor, returns their bounding boxes.\n",
    "\n",
    "    Parameters:\n",
    "        frame: Current video frame.\n",
    "        background_subtractor: Background subtractor for motion detection.\n",
    "        area_threshold: Minimum area for detected bounding boxes.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        List of detected bounding boxes with their areas, [x1, y1, x2, y2, area]\n",
    "\n",
    "    Notes:\n",
    "        - https://medium.com/analytics-vidhya/opencv-findcontours-detailed-guide-692ee19eeb18\n",
    "    \"\"\"\n",
    "\n",
    "    fg_mask = background_subtractor.apply(frame)\n",
    "    contours, _ = cv.findContours(fg_mask, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
    "    detections = []\n",
    "    for cnt in contours:\n",
    "        x, y, w, h = cv.boundingRect(cnt)\n",
    "        area = w * h\n",
    "        if area > area_threshold:\n",
    "            detections.append([x, y, x + w, y + h, area])\n",
    "\n",
    "    return detections, fg_mask"
   ],
   "id": "1119c0ce7b38687d",
   "outputs": [],
   "execution_count": 101
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T21:37:04.873219Z",
     "start_time": "2025-01-19T21:37:04.870358Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# utility function\n",
    "def mog2_movement_detection(frame: np.ndarray, *, background_subtractor: cv.BackgroundSubtractor, area_threshold: int = 100, overlap_threshold = 0.0, draw=False) -> tuple[Any, list, np.ndarray]:\n",
    "        detections, _ = detect_moving_objects(frame, background_subtractor, area_threshold=area_threshold)\n",
    "        merged_detections = merge_overlapping_detections(detections, overlap_threshold)\n",
    "\n",
    "        # Draw bounding boxes\n",
    "        if draw:\n",
    "            for det in merged_detections:\n",
    "                x1, y1, x2, y2, _ = det\n",
    "                cv.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "        return detections, merged_detections, frame"
   ],
   "id": "fc0a2c89d9a3cc3c",
   "outputs": [],
   "execution_count": 102
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## YOLO11 People Detection",
   "id": "e0cc2c503497ac20"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T21:37:04.920451Z",
     "start_time": "2025-01-19T21:37:04.916332Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PeopleDetector(YOLO):\n",
    "    \"\"\"\n",
    "    0: 320x640 1 person, 1 umbrella, 3 chairs, 1 couch, 3 potted plants, 2 tvs, 21.1ms\n",
    "    Speed: 1.2ms preprocess, 21.1ms inference, 1.1ms postprocess per image at shape (1, 3, 320, 640)\n",
    "    [ultralytics.engine.results.Results object with attributes:\n",
    "\n",
    "    boxes: ultralytics.engine.results.Boxes object\n",
    "    keypoints: None\n",
    "    masks: None\n",
    "    names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
    "    obb: None\n",
    "    orig_img: array([[[  0,   1,   1],\n",
    "        [ 93, 102, 103],\n",
    "        [ 93, 102, 103],\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name, threshold=0.25,  **kwargs):\n",
    "        super().__init__(model_name, **kwargs)\n",
    "        self.focus_on_classes = [0]\n",
    "        self.threshold = threshold\n",
    "\n",
    "\n",
    "    def detect(self, frame: np.ndarray) -> tuple[np.array, np.array, Any]:\n",
    "        \"\"\"\n",
    "        :param frame: frame in which detect people\n",
    "        :return: a np.array of the confidences scores, a np.array of the bounding box coordinates, and the result obj\n",
    "        \"\"\"\n",
    "        results = self(frame, classes=self.focus_on_classes, conf=self.threshold, device=self.device)[0] # list of 1 Results object, because we can predict in batches (for video only)\n",
    "        \"\"\"\n",
    "        cls: tensor([0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
    "        conf: tensor([0.9429, 0.9262, 0.8841, 0.8833, 0.8824, 0.8773], device='cuda:0')\n",
    "        xywh: tensor([\n",
    "            [1207.5342,  576.8246,  510.5572,  977.0013],\n",
    "            [ 447.3520,  351.2901,  267.4479,  537.2728],\n",
    "            [ 709.4022,  370.9929,  235.7747,  489.7085],\n",
    "            [ 317.8023,  557.6183,  408.7670,  422.3871],\n",
    "            [1684.3063,  498.2565,  354.5173,  403.8601],\n",
    "            [1512.9242,  217.1133,  132.6541,  321.3105]\n",
    "        ])\n",
    "        \"\"\"\n",
    "        # print(results)\n",
    "        return results.boxes.conf.numpy(), results.boxes.xywh.numpy(), results\n",
    "\n",
    "    def detect_on_frames(self, frames: list[np.ndarray]) -> list[tuple[np.array, np.array, Any]]:\n",
    "        return [\n",
    "            self.detect(frame) for frame in frames\n",
    "        ]\n",
    "\n",
    "\n"
   ],
   "id": "4d9194314ee27323",
   "outputs": [],
   "execution_count": 103
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## PROCESS VIDEO\n",
    "*Here happens the magick*\n"
   ],
   "id": "e2447958e18206ed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T21:43:10.020839Z",
     "start_time": "2025-01-19T21:43:10.016676Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_video(video_path: str, people_detector: PeopleDetector, scale: float = 0.5, overlap_threshold: float = 0.3, area_threshold: int = 100, early_stop=None):\n",
    "    \"\"\"\n",
    "    Processes the input video.\n",
    "    Parameters:\n",
    "        video_path: Path to the video file.\n",
    "        scale: Scaling factor for resizing frames.\n",
    "        overlap_threshold: Threshold for merging overlapping detections using IoU.\n",
    "        area_threshold: Minimum area for detected bounding boxes.\n",
    "        people_detector: YOLO model to detect people.\n",
    "        early_stop: stop after n frames\n",
    "    \"\"\"\n",
    "    capture = cv.VideoCapture(video_path)\n",
    "    back_sub = cv.createBackgroundSubtractorMOG2(detectShadows=False)\n",
    "    iterations = 0\n",
    "    while early_stop is not None and iterations < early_stop:\n",
    "        iterations += 1\n",
    "        ret, frame = capture.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = cv.resize(frame, None, fx=scale, fy=scale, interpolation=cv.INTER_LINEAR)\n",
    "\n",
    "        detections, merged_detections, frame = mog2_movement_detection(frame, background_subtractor=back_sub, area_threshold=area_threshold,\n",
    "                                overlap_threshold=overlap_threshold, draw=False)\n",
    "\n",
    "        # crop frames on bboxes\n",
    "        cropped_frames = crop_bboxes(frame, bboxes=[md[:-1] for md in merged_detections])\n",
    "\n",
    "        # detect people on cropped frames\n",
    "\n",
    "        # probs, bboxes, result = people_detector.detect(frame)\n",
    "        # annotated_frame = result.plot()\n",
    "        # view(annotated_frame)\n",
    "\n",
    "        detections = people_detector.detect_on_frames(cropped_frames)\n",
    "        for detection in detections:\n",
    "            probs, _, result = detection\n",
    "            print(probs)\n",
    "            annotated_frame = result.plot()\n",
    "            view(annotated_frame)\n",
    "\n",
    "        # for frame in cropped_frames:\n",
    "        #     stop = view(frame, scale=scale)\n",
    "        #     if not stop:\n",
    "        #         break\n",
    "    else:\n",
    "        print(f\"Stopped after {iterations} frames due early stopping condition.\")\n",
    "\n",
    "    capture.release()\n",
    "    cv.destroyAllWindows()\n",
    "\n",
    "    if sys.platform == 'darwin':\n",
    "        for _ in range(4):\n",
    "            cv.waitKey(1)\n",
    "\n",
    "    cv.destroyAllWindows()\n",
    "\n"
   ],
   "id": "8a3902c9f9ae4861",
   "outputs": [],
   "execution_count": 112
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Main",
   "id": "b8d04d81258a7f91"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T21:37:05.178675Z",
     "start_time": "2025-01-19T21:37:05.011445Z"
    }
   },
   "cell_type": "code",
   "source": [
    "yolosize = 'n'\n",
    "yolo11 = PeopleDetector(f\"yolo11{yolosize}.pt\", verbose=False)\n",
    "yolo11.to('cpu')\n",
    "\n",
    "probs, bboxes, _ = yolo11.detect(\"The-Office-HD-Background.jpg\")\n",
    "print('confidence scores', probs)\n",
    "print('bboxes', bboxes)"
   ],
   "id": "c51cdddc4972c528",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/tommaso/PycharmProjects/ComputerVision/The-Office-HD-Background.jpg: 384x640 6 persons, 61.3ms\n",
      "Speed: 1.4ms preprocess, 61.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "confidence scores [    0.93549     0.90471     0.89169     0.86111     0.80327     0.75666]\n",
      "bboxes [[     1204.7       577.6      508.52      981.28]\n",
      " [     447.91      353.48      269.95      532.36]\n",
      " [     707.89      367.57      237.81      477.16]\n",
      " [     306.16      557.33      395.06      414.31]\n",
      " [     1684.8      497.84      352.05      400.59]\n",
      " [     1509.9      214.15      127.53      313.29]]\n"
     ]
    }
   ],
   "execution_count": 105
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T21:43:33.527269Z",
     "start_time": "2025-01-19T21:43:14.785564Z"
    }
   },
   "cell_type": "code",
   "source": [
    "video_path = 'SamsungGear360.mp4'\n",
    "process_video(video_path, yolo11, scale=0.5, overlap_threshold=0.0005, area_threshold=700, early_stop=1000)"
   ],
   "id": "3946034d83bc6843",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 320x640 (no detections), 58.3ms\n",
      "Speed: 1.8ms preprocess, 58.3ms inference, 0.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "[]\n",
      "\n",
      "0: 640x32 (no detections), 15.5ms\n",
      "Speed: 0.2ms preprocess, 15.5ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 32)\n",
      "[]\n",
      "\n",
      "0: 640x32 (no detections), 13.4ms\n",
      "Speed: 0.3ms preprocess, 13.4ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 32)\n",
      "[]\n",
      "\n",
      "0: 640x64 (no detections), 21.3ms\n",
      "Speed: 0.4ms preprocess, 21.3ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 64)\n",
      "[]\n",
      "\n",
      "0: 640x64 (no detections), 17.6ms\n",
      "Speed: 0.4ms preprocess, 17.6ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 64)\n",
      "[]\n",
      "\n",
      "0: 640x128 (no detections), 28.9ms\n",
      "Speed: 0.6ms preprocess, 28.9ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 128)\n",
      "[]\n",
      "\n",
      "0: 640x128 (no detections), 25.9ms\n",
      "Speed: 0.5ms preprocess, 25.9ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 128)\n",
      "[]\n",
      "\n",
      "0: 640x128 (no detections), 26.3ms\n",
      "Speed: 0.5ms preprocess, 26.3ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 128)\n",
      "[]\n",
      "\n",
      "0: 640x128 (no detections), 27.0ms\n",
      "Speed: 0.5ms preprocess, 27.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 128)\n",
      "[]\n",
      "\n",
      "0: 640x128 (no detections), 26.6ms\n",
      "Speed: 0.6ms preprocess, 26.6ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 128)\n",
      "[]\n",
      "\n",
      "0: 640x128 (no detections), 26.5ms\n",
      "Speed: 0.6ms preprocess, 26.5ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 128)\n",
      "[]\n",
      "\n",
      "0: 640x128 (no detections), 25.8ms\n",
      "Speed: 0.5ms preprocess, 25.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 128)\n",
      "[]\n",
      "\n",
      "0: 640x128 (no detections), 26.1ms\n",
      "Speed: 0.5ms preprocess, 26.1ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 128)\n",
      "\n",
      "0: 640x416 (no detections), 64.4ms\n",
      "Speed: 1.3ms preprocess, 64.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 416)\n",
      "[]\n",
      "[]\n",
      "\n",
      "0: 640x160 (no detections), 30.9ms\n",
      "Speed: 0.6ms preprocess, 30.9ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 160)\n",
      "\n",
      "0: 640x224 (no detections), 39.2ms\n",
      "Speed: 0.7ms preprocess, 39.2ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 224)\n",
      "[]\n",
      "[]\n",
      "\n",
      "0: 640x160 (no detections), 30.6ms\n",
      "Speed: 0.6ms preprocess, 30.6ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 160)\n",
      "\n",
      "0: 640x224 (no detections), 39.6ms\n",
      "Speed: 0.8ms preprocess, 39.6ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 224)\n",
      "[]\n",
      "[]\n",
      "\n",
      "0: 640x160 (no detections), 30.7ms\n",
      "Speed: 0.8ms preprocess, 30.7ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 160)\n",
      "\n",
      "0: 640x224 (no detections), 40.6ms\n",
      "Speed: 1.0ms preprocess, 40.6ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 224)\n",
      "[]\n",
      "[]\n",
      "\n",
      "0: 640x192 (no detections), 35.8ms\n",
      "Speed: 0.6ms preprocess, 35.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 192)\n",
      "\n",
      "0: 640x224 (no detections), 41.2ms\n",
      "Speed: 0.7ms preprocess, 41.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 224)\n",
      "[]\n",
      "[]\n",
      "\n",
      "0: 640x192 (no detections), 36.0ms\n",
      "Speed: 0.7ms preprocess, 36.0ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 192)\n",
      "\n",
      "0: 640x256 (no detections), 47.0ms\n",
      "Speed: 0.7ms preprocess, 47.0ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 256)\n",
      "[]\n",
      "[]\n",
      "\n",
      "0: 640x192 (no detections), 36.2ms\n",
      "Speed: 0.7ms preprocess, 36.2ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 192)\n",
      "\n",
      "0: 640x224 (no detections), 41.5ms\n",
      "Speed: 0.7ms preprocess, 41.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 224)\n",
      "[]\n",
      "[]\n",
      "\n",
      "0: 640x224 (no detections), 41.8ms\n",
      "Speed: 0.7ms preprocess, 41.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 224)\n",
      "\n",
      "0: 640x256 (no detections), 46.6ms\n",
      "Speed: 0.8ms preprocess, 46.6ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 256)\n",
      "\n",
      "0: 448x640 (no detections), 79.6ms\n",
      "Speed: 1.1ms preprocess, 79.6ms inference, 0.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "\n",
      "0: 640x224 1 person, 42.1ms\n",
      "Speed: 0.8ms preprocess, 42.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 224)\n",
      "\n",
      "0: 640x256 (no detections), 46.7ms\n",
      "Speed: 0.8ms preprocess, 46.7ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 256)\n",
      "[    0.73269]\n",
      "[]\n",
      "\n",
      "0: 640x224 1 person, 42.8ms\n",
      "Speed: 0.8ms preprocess, 42.8ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 224)\n",
      "\n",
      "0: 480x640 (no detections), 82.9ms\n",
      "Speed: 1.2ms preprocess, 82.9ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 416x640 (no detections), 70.9ms\n",
      "Speed: 1.4ms preprocess, 70.9ms inference, 0.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "[     0.8098]\n",
      "[]\n",
      "[]\n",
      "\n",
      "0: 640x256 1 person, 44.9ms\n",
      "Speed: 1.2ms preprocess, 44.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 256)\n",
      "\n",
      "0: 640x544 (no detections), 89.5ms\n",
      "Speed: 1.5ms preprocess, 89.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 544)\n",
      "[     0.7283]\n",
      "[]\n",
      "\n",
      "0: 640x256 1 person, 44.7ms\n",
      "Speed: 0.8ms preprocess, 44.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 256)\n",
      "\n",
      "0: 384x640 (no detections), 67.2ms\n",
      "Speed: 1.3ms preprocess, 67.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x480 (no detections), 80.6ms\n",
      "Speed: 1.5ms preprocess, 80.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "[    0.78682]\n",
      "[]\n",
      "[]\n",
      "\n",
      "0: 640x256 1 person, 44.5ms\n",
      "Speed: 0.9ms preprocess, 44.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 256)\n",
      "\n",
      "0: 416x640 (no detections), 64.7ms\n",
      "Speed: 1.1ms preprocess, 64.7ms inference, 0.3ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 640x576 (no detections), 94.1ms\n",
      "Speed: 1.6ms preprocess, 94.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 576)\n",
      "[    0.81917]\n",
      "[]\n",
      "[]\n",
      "\n",
      "0: 640x256 1 person, 44.9ms\n",
      "Speed: 0.9ms preprocess, 44.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 256)\n",
      "\n",
      "0: 448x640 (no detections), 71.0ms\n",
      "Speed: 1.3ms preprocess, 71.0ms inference, 0.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "[    0.84537]\n",
      "[]\n",
      "\n",
      "0: 640x256 1 person, 44.4ms\n",
      "Speed: 1.1ms preprocess, 44.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 256)\n",
      "\n",
      "0: 480x640 (no detections), 74.5ms\n",
      "Speed: 1.3ms preprocess, 74.5ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[    0.86558]\n",
      "[]\n",
      "\n",
      "0: 640x256 1 person, 44.1ms\n",
      "Speed: 0.8ms preprocess, 44.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 256)\n",
      "\n",
      "0: 640x608 (no detections), 99.1ms\n",
      "Speed: 1.7ms preprocess, 99.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 608)\n",
      "[    0.87021]\n",
      "[]\n",
      "\n",
      "0: 640x224 1 person, 39.5ms\n",
      "Speed: 0.7ms preprocess, 39.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 224)\n",
      "\n",
      "0: 640x576 (no detections), 88.3ms\n",
      "Speed: 1.6ms preprocess, 88.3ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 576)\n",
      "[    0.85915]\n",
      "[]\n",
      "\n",
      "0: 640x192 1 person, 35.5ms\n",
      "Speed: 0.7ms preprocess, 35.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 192)\n",
      "\n",
      "0: 640x512 (no detections), 84.6ms\n",
      "Speed: 1.6ms preprocess, 84.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 512)\n",
      "[     0.8814]\n",
      "[]\n",
      "\n",
      "0: 640x192 1 person, 34.6ms\n",
      "Speed: 0.7ms preprocess, 34.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 192)\n",
      "\n",
      "0: 640x512 (no detections), 79.0ms\n",
      "Speed: 1.4ms preprocess, 79.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 512)\n",
      "[     0.8592]\n",
      "[]\n",
      "\n",
      "0: 640x256 1 person, 43.5ms\n",
      "Speed: 0.8ms preprocess, 43.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 256)\n",
      "[    0.87492]\n",
      "\n",
      "0: 640x192 1 person, 34.2ms\n",
      "Speed: 0.6ms preprocess, 34.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 192)\n",
      "\n",
      "0: 640x512 (no detections), 80.6ms\n",
      "Speed: 1.6ms preprocess, 80.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 512)\n",
      "[    0.87952]\n",
      "[]\n",
      "\n",
      "0: 640x256 1 person, 43.9ms\n",
      "Speed: 0.9ms preprocess, 43.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 256)\n",
      "\n",
      "0: 576x640 (no detections), 94.8ms\n",
      "Speed: 1.6ms preprocess, 94.8ms inference, 0.4ms postprocess per image at shape (1, 3, 576, 640)\n",
      "[    0.87197]\n",
      "[]\n",
      "\n",
      "0: 640x224 1 person, 39.8ms\n",
      "Speed: 0.8ms preprocess, 39.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 224)\n",
      "\n",
      "0: 640x224 (no detections), 39.2ms\n",
      "Speed: 0.7ms preprocess, 39.2ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 224)\n",
      "\n",
      "0: 544x640 (no detections), 91.0ms\n",
      "Speed: 1.4ms preprocess, 91.0ms inference, 0.4ms postprocess per image at shape (1, 3, 544, 640)\n",
      "[    0.86273]\n",
      "[]\n",
      "[]\n",
      "\n",
      "0: 640x288 1 person, 56.0ms\n",
      "Speed: 0.9ms preprocess, 56.0ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 288)\n",
      "\n",
      "0: 640x576 (no detections), 93.4ms\n",
      "Speed: 2.1ms preprocess, 93.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 576)\n",
      "[    0.86242]\n",
      "[]\n",
      "\n",
      "0: 640x256 1 person, 47.2ms\n",
      "Speed: 0.9ms preprocess, 47.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 256)\n",
      "\n",
      "0: 544x640 (no detections), 87.1ms\n",
      "Speed: 1.7ms preprocess, 87.1ms inference, 0.4ms postprocess per image at shape (1, 3, 544, 640)\n",
      "[    0.85561]\n",
      "[]\n",
      "\n",
      "0: 640x224 1 person, 42.7ms\n",
      "Speed: 0.8ms preprocess, 42.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 224)\n",
      "\n",
      "0: 480x640 (no detections), 78.8ms\n",
      "Speed: 1.4ms preprocess, 78.8ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[    0.88898]\n",
      "[]\n",
      "\n",
      "0: 640x288 1 person, 50.7ms\n",
      "Speed: 0.9ms preprocess, 50.7ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 288)\n",
      "[    0.86272]\n",
      "\n",
      "0: 640x288 1 person, 49.2ms\n",
      "Speed: 0.9ms preprocess, 49.2ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 288)\n",
      "[    0.87932]\n",
      "\n",
      "0: 640x256 1 person, 46.0ms\n",
      "Speed: 0.9ms preprocess, 46.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 256)\n",
      "[    0.88937]\n",
      "\n",
      "0: 640x320 1 person, 59.2ms\n",
      "Speed: 1.1ms preprocess, 59.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 320)\n",
      "\n",
      "0: 480x640 (no detections), 72.2ms\n",
      "Speed: 1.3ms preprocess, 72.2ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[    0.84835]\n",
      "[]\n",
      "\n",
      "0: 640x256 1 person, 44.8ms\n",
      "Speed: 1.0ms preprocess, 44.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 256)\n",
      "\n",
      "0: 480x640 (no detections), 73.4ms\n",
      "Speed: 1.3ms preprocess, 73.4ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[     0.8362]\n",
      "[]\n",
      "\n",
      "0: 640x192 1 person, 35.1ms\n",
      "Speed: 0.6ms preprocess, 35.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 192)\n",
      "\n",
      "0: 640x320 (no detections), 52.2ms\n",
      "Speed: 1.0ms preprocess, 52.2ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 320)\n",
      "\n",
      "0: 640x544 (no detections), 83.2ms\n",
      "Speed: 1.5ms preprocess, 83.2ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 544)\n",
      "[    0.81524]\n",
      "[]\n",
      "[]\n",
      "\n",
      "0: 640x320 (no detections), 54.8ms\n",
      "Speed: 1.0ms preprocess, 54.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 320)\n",
      "\n",
      "0: 640x320 (no detections), 53.3ms\n",
      "Speed: 1.1ms preprocess, 53.3ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 320)\n",
      "\n",
      "0: 640x320 1 person, 53.2ms\n",
      "Speed: 0.9ms preprocess, 53.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 320)\n",
      "\n",
      "0: 640x416 1 person, 70.1ms\n",
      "Speed: 1.1ms preprocess, 70.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 416)\n",
      "[]\n",
      "[]\n",
      "[    0.72549]\n",
      "[    0.28877]\n",
      "\n",
      "0: 640x320 1 person, 52.1ms\n",
      "Speed: 1.0ms preprocess, 52.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 320)\n",
      "\n",
      "0: 640x320 (no detections), 52.7ms\n",
      "Speed: 1.1ms preprocess, 52.7ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 320)\n",
      "\n",
      "0: 640x320 1 person, 54.1ms\n",
      "Speed: 0.9ms preprocess, 54.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 320)\n",
      "\n",
      "0: 640x576 (no detections), 87.0ms\n",
      "Speed: 1.6ms preprocess, 87.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 576)\n",
      "[    0.36432]\n",
      "[]\n",
      "[    0.32298]\n",
      "[]\n",
      "\n",
      "0: 640x224 1 person, 41.0ms\n",
      "Speed: 0.8ms preprocess, 41.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 224)\n",
      "\n",
      "0: 640x288 (no detections), 48.5ms\n",
      "Speed: 1.0ms preprocess, 48.5ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 288)\n",
      "\n",
      "0: 320x640 (no detections), 57.5ms\n",
      "Speed: 0.9ms preprocess, 57.5ms inference, 0.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "[    0.83262]\n",
      "[]\n",
      "[]\n",
      "\n",
      "0: 640x128 1 person, 30.5ms\n",
      "Speed: 0.5ms preprocess, 30.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 128)\n",
      "\n",
      "0: 640x288 (no detections), 48.0ms\n",
      "Speed: 0.9ms preprocess, 48.0ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 288)\n",
      "\n",
      "0: 640x320 1 person, 53.1ms\n",
      "Speed: 1.0ms preprocess, 53.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 320)\n",
      "[    0.64036]\n",
      "[]\n",
      "[    0.43216]\n",
      "\n",
      "0: 480x640 (no detections), 73.9ms\n",
      "Speed: 1.4ms preprocess, 73.9ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x448 (no detections), 75.5ms\n",
      "Speed: 1.2ms preprocess, 75.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x288 1 person, 49.3ms\n",
      "Speed: 0.8ms preprocess, 49.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 288)\n",
      "[]\n",
      "[]\n",
      "[    0.35313]\n",
      "\n",
      "0: 544x640 (no detections), 83.1ms\n",
      "Speed: 1.5ms preprocess, 83.1ms inference, 0.4ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 640x160 1 person, 35.2ms\n",
      "Speed: 0.6ms preprocess, 35.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 160)\n",
      "\n",
      "0: 640x288 1 person, 48.6ms\n",
      "Speed: 0.8ms preprocess, 48.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 288)\n",
      "\n",
      "0: 576x640 (no detections), 85.4ms\n",
      "Speed: 1.7ms preprocess, 85.4ms inference, 0.4ms postprocess per image at shape (1, 3, 576, 640)\n",
      "[]\n",
      "[    0.63884]\n",
      "[    0.30702]\n",
      "[]\n",
      "\n",
      "0: 640x640 (no detections), 103.0ms\n",
      "Speed: 1.8ms preprocess, 103.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x224 1 person, 39.4ms\n",
      "Speed: 0.7ms preprocess, 39.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 224)\n",
      "\n",
      "0: 640x288 1 person, 48.9ms\n",
      "Speed: 0.8ms preprocess, 48.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 288)\n",
      "[]\n",
      "[    0.72357]\n",
      "[    0.54635]\n",
      "\n",
      "0: 640x192 1 person, 36.3ms\n",
      "Speed: 0.7ms preprocess, 36.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 192)\n",
      "\n",
      "0: 640x576 (no detections), 88.2ms\n",
      "Speed: 1.7ms preprocess, 88.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 576)\n",
      "\n",
      "0: 640x320 1 person, 53.8ms\n",
      "Speed: 0.9ms preprocess, 53.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 320)\n",
      "[    0.51341]\n",
      "[]\n",
      "[    0.57541]\n",
      "\n",
      "0: 640x448 (no detections), 70.4ms\n",
      "Speed: 1.3ms preprocess, 70.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x224 1 person, 40.1ms\n",
      "Speed: 0.7ms preprocess, 40.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 224)\n",
      "\n",
      "0: 640x320 1 person, 54.1ms\n",
      "Speed: 0.9ms preprocess, 54.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 320)\n",
      "[]\n",
      "[    0.66146]\n",
      "[    0.52058]\n",
      "\n",
      "0: 640x192 1 person, 36.8ms\n",
      "Speed: 0.8ms preprocess, 36.8ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 192)\n",
      "[    0.81508]\n",
      "\n",
      "0: 640x192 1 person, 34.5ms\n",
      "Speed: 0.7ms preprocess, 34.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 192)\n",
      "[    0.67787]\n",
      "\n",
      "0: 640x192 1 person, 34.7ms\n",
      "Speed: 0.7ms preprocess, 34.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 192)\n",
      "[    0.75437]\n",
      "\n",
      "0: 640x192 1 person, 34.5ms\n",
      "Speed: 0.7ms preprocess, 34.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 192)\n",
      "[    0.68001]\n",
      "\n",
      "0: 640x224 1 person, 39.8ms\n",
      "Speed: 0.7ms preprocess, 39.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 224)\n",
      "[    0.56925]\n",
      "\n",
      "0: 640x160 1 person, 31.0ms\n",
      "Speed: 0.7ms preprocess, 31.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 160)\n",
      "\n",
      "0: 640x192 1 person, 35.4ms\n",
      "Speed: 0.7ms preprocess, 35.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 192)\n",
      "[    0.78402]\n",
      "[    0.26319]\n",
      "\n",
      "0: 640x192 1 person, 34.7ms\n",
      "Speed: 0.7ms preprocess, 34.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 192)\n",
      "\n",
      "0: 640x224 (no detections), 40.2ms\n",
      "Speed: 0.9ms preprocess, 40.2ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 224)\n",
      "[    0.63581]\n",
      "[]\n",
      "\n",
      "0: 640x192 1 person, 35.0ms\n",
      "Speed: 0.7ms preprocess, 35.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 192)\n",
      "[    0.51226]\n",
      "\n",
      "0: 640x192 1 person, 35.5ms\n",
      "Speed: 0.8ms preprocess, 35.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 192)\n",
      "\n",
      "0: 640x160 (no detections), 30.7ms\n",
      "Speed: 0.6ms preprocess, 30.7ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 160)\n",
      "[    0.71871]\n",
      "[]\n",
      "\n",
      "0: 640x160 1 person, 30.2ms\n",
      "Speed: 0.6ms preprocess, 30.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 160)\n",
      "\n",
      "0: 640x192 (no detections), 34.7ms\n",
      "Speed: 0.6ms preprocess, 34.7ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 192)\n",
      "\n",
      "0: 640x288 (no detections), 49.5ms\n",
      "Speed: 0.8ms preprocess, 49.5ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 288)\n",
      "[    0.73659]\n",
      "[]\n",
      "[]\n",
      "\n",
      "0: 640x128 1 person, 26.9ms\n",
      "Speed: 0.5ms preprocess, 26.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 128)\n",
      "\n",
      "0: 640x160 (no detections), 31.2ms\n",
      "Speed: 0.5ms preprocess, 31.2ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 160)\n",
      "\n",
      "0: 640x288 (no detections), 49.3ms\n",
      "Speed: 0.8ms preprocess, 49.3ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 288)\n",
      "[    0.75728]\n",
      "[]\n",
      "[]\n",
      "\n",
      "0: 640x96 1 person, 25.6ms\n",
      "Speed: 0.5ms preprocess, 25.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 96)\n",
      "\n",
      "0: 640x224 (no detections), 39.7ms\n",
      "Speed: 0.7ms preprocess, 39.7ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 224)\n",
      "\n",
      "0: 640x352 (no detections), 64.6ms\n",
      "Speed: 1.1ms preprocess, 64.6ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 224x640 (no detections), 44.3ms\n",
      "Speed: 1.0ms preprocess, 44.3ms inference, 0.3ms postprocess per image at shape (1, 3, 224, 640)\n",
      "[     0.5647]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "\n",
      "0: 640x96 (no detections), 22.7ms\n",
      "Speed: 0.5ms preprocess, 22.7ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 96)\n",
      "\n",
      "0: 640x320 (no detections), 53.8ms\n",
      "Speed: 0.9ms preprocess, 53.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 320)\n",
      "\n",
      "0: 640x384 (no detections), 70.2ms\n",
      "Speed: 1.0ms preprocess, 70.2ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x128 (no detections), 27.6ms\n",
      "Speed: 0.5ms preprocess, 27.6ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 128)\n",
      "\n",
      "0: 288x640 (no detections), 57.0ms\n",
      "Speed: 1.0ms preprocess, 57.0ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 640)\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "\n",
      "0: 640x640 (no detections), 98.1ms\n",
      "Speed: 1.8ms preprocess, 98.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x288 (no detections), 50.9ms\n",
      "Speed: 0.9ms preprocess, 50.9ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 288)\n",
      "\n",
      "0: 640x256 (no detections), 50.3ms\n",
      "Speed: 0.8ms preprocess, 50.3ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 256)\n",
      "\n",
      "0: 640x128 (no detections), 26.2ms\n",
      "Speed: 0.5ms preprocess, 26.2ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 128)\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "\n",
      "0: 640x288 1 person, 50.5ms\n",
      "Speed: 0.9ms preprocess, 50.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 288)\n",
      "\n",
      "0: 640x608 (no detections), 103.5ms\n",
      "Speed: 1.8ms preprocess, 103.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 608)\n",
      "\n",
      "0: 224x640 (no detections), 38.7ms\n",
      "Speed: 0.7ms preprocess, 38.7ms inference, 0.3ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 640x96 (no detections), 22.2ms\n",
      "Speed: 0.5ms preprocess, 22.2ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 96)\n",
      "\n",
      "0: 640x192 (no detections), 34.8ms\n",
      "Speed: 0.7ms preprocess, 34.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 192)\n",
      "[    0.62719]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "\n",
      "0: 640x352 (no detections), 58.6ms\n",
      "Speed: 1.3ms preprocess, 58.6ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x416 (no detections), 74.5ms\n",
      "Speed: 1.1ms preprocess, 74.5ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "0: 224x640 (no detections), 38.1ms\n",
      "Speed: 0.9ms preprocess, 38.1ms inference, 0.3ms postprocess per image at shape (1, 3, 224, 640)\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "\n",
      "0: 640x192 1 person, 34.9ms\n",
      "Speed: 0.6ms preprocess, 34.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 192)\n",
      "\n",
      "0: 640x448 (no detections), 73.5ms\n",
      "Speed: 1.2ms preprocess, 73.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x480 (no detections), 84.0ms\n",
      "Speed: 1.3ms preprocess, 84.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "[    0.47979]\n",
      "[]\n",
      "[]\n",
      "\n",
      "0: 640x160 1 person, 31.4ms\n",
      "Speed: 0.6ms preprocess, 31.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 160)\n",
      "\n",
      "0: 640x448 (no detections), 70.2ms\n",
      "Speed: 1.2ms preprocess, 70.2ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x448 (no detections), 71.5ms\n",
      "Speed: 1.3ms preprocess, 71.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "[    0.35125]\n",
      "[]\n",
      "[]\n",
      "\n",
      "0: 640x192 1 person, 36.5ms\n",
      "Speed: 0.6ms preprocess, 36.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 192)\n",
      "\n",
      "0: 640x448 (no detections), 73.7ms\n",
      "Speed: 1.4ms preprocess, 73.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x448 (no detections), 73.7ms\n",
      "Speed: 1.3ms preprocess, 73.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 448)\n",
      "[    0.47205]\n",
      "[]\n",
      "[]\n",
      "\n",
      "0: 640x160 1 person, 31.5ms\n",
      "Speed: 0.6ms preprocess, 31.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 160)\n",
      "\n",
      "0: 640x416 (no detections), 65.4ms\n",
      "Speed: 1.1ms preprocess, 65.4ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "0: 640x448 (no detections), 69.9ms\n",
      "Speed: 1.2ms preprocess, 69.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "[     0.2767]\n",
      "[]\n",
      "[]\n",
      "\n",
      "0: 640x224 1 person, 40.0ms\n",
      "Speed: 0.8ms preprocess, 40.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 224)\n",
      "\n",
      "0: 640x192 (no detections), 34.7ms\n",
      "Speed: 0.8ms preprocess, 34.7ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 192)\n",
      "\n",
      "0: 640x416 (no detections), 68.5ms\n",
      "Speed: 1.1ms preprocess, 68.5ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 416)\n",
      "[    0.32394]\n",
      "[]\n",
      "[]\n",
      "\n",
      "0: 640x128 (no detections), 26.9ms\n",
      "Speed: 0.5ms preprocess, 26.9ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 128)\n",
      "\n",
      "0: 640x416 (no detections), 66.2ms\n",
      "Speed: 1.2ms preprocess, 66.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "0: 640x128 (no detections), 26.8ms\n",
      "Speed: 0.5ms preprocess, 26.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 128)\n",
      "\n",
      "0: 640x416 (no detections), 64.7ms\n",
      "Speed: 1.2ms preprocess, 64.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 416)\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "\n",
      "0: 640x352 1 person, 57.4ms\n",
      "Speed: 1.0ms preprocess, 57.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x448 (no detections), 71.7ms\n",
      "Speed: 1.2ms preprocess, 71.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "[    0.73349]\n",
      "[]\n",
      "\n",
      "0: 640x192 1 person, 36.2ms\n",
      "Speed: 0.6ms preprocess, 36.2ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 192)\n",
      "\n",
      "0: 640x160 1 person, 32.6ms\n",
      "Speed: 0.6ms preprocess, 32.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 160)\n",
      "\n",
      "0: 640x448 (no detections), 71.8ms\n",
      "Speed: 1.1ms preprocess, 71.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 448)\n",
      "[    0.55807]\n",
      "[    0.28361]\n",
      "[]\n",
      "\n",
      "0: 544x640 (no detections), 89.0ms\n",
      "Speed: 1.8ms preprocess, 89.0ms inference, 0.4ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 640x448 (no detections), 71.8ms\n",
      "Speed: 1.2ms preprocess, 71.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 416x640 (no detections), 72.4ms\n",
      "Speed: 1.3ms preprocess, 72.4ms inference, 0.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "\n",
      "0: 640x224 1 person, 39.9ms\n",
      "Speed: 0.7ms preprocess, 39.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 224)\n",
      "\n",
      "0: 640x448 (no detections), 71.2ms\n",
      "Speed: 1.4ms preprocess, 71.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 416x640 (no detections), 65.0ms\n",
      "Speed: 1.1ms preprocess, 65.0ms inference, 0.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "[    0.41363]\n",
      "[]\n",
      "[]\n",
      "\n",
      "0: 640x416 1 person, 66.8ms\n",
      "Speed: 1.2ms preprocess, 66.8ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "0: 640x416 (no detections), 65.8ms\n",
      "Speed: 1.4ms preprocess, 65.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "0: 416x640 (no detections), 64.6ms\n",
      "Speed: 1.2ms preprocess, 64.6ms inference, 0.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "[    0.61608]\n",
      "[]\n",
      "[]\n",
      "\n",
      "0: 640x320 1 person, 59.9ms\n",
      "Speed: 0.9ms preprocess, 59.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 320)\n",
      "\n",
      "0: 640x448 (no detections), 70.5ms\n",
      "Speed: 1.2ms preprocess, 70.5ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 416x640 (no detections), 64.7ms\n",
      "Speed: 1.1ms preprocess, 64.7ms inference, 0.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 640x160 (no detections), 30.7ms\n",
      "Speed: 0.6ms preprocess, 30.7ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 160)\n",
      "[     0.6546]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "\n",
      "0: 512x640 (no detections), 86.3ms\n",
      "Speed: 1.5ms preprocess, 86.3ms inference, 0.3ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 640x448 (no detections), 71.9ms\n",
      "Speed: 1.2ms preprocess, 71.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 320x640 (no detections), 57.1ms\n",
      "Speed: 0.9ms preprocess, 57.1ms inference, 0.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "\n",
      "0: 512x640 (no detections), 80.5ms\n",
      "Speed: 1.5ms preprocess, 80.5ms inference, 0.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 640x96 (no detections), 22.3ms\n",
      "Speed: 0.4ms preprocess, 22.3ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 96)\n",
      "\n",
      "0: 640x544 (no detections), 90.2ms\n",
      "Speed: 1.4ms preprocess, 90.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 320x640 (no detections), 61.2ms\n",
      "Speed: 0.9ms preprocess, 61.2ms inference, 0.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "\n",
      "0: 640x288 1 person, 48.0ms\n",
      "Speed: 0.9ms preprocess, 48.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 288)\n",
      "\n",
      "0: 640x608 (no detections), 91.6ms\n",
      "Speed: 1.6ms preprocess, 91.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 608)\n",
      "\n",
      "0: 352x640 (no detections), 61.9ms\n",
      "Speed: 1.1ms preprocess, 61.9ms inference, 0.3ms postprocess per image at shape (1, 3, 352, 640)\n",
      "[    0.58753]\n",
      "[]\n",
      "[]\n",
      "\n",
      "0: 640x448 1 person, 70.0ms\n",
      "Speed: 1.4ms preprocess, 70.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x608 (no detections), 90.5ms\n",
      "Speed: 2.0ms preprocess, 90.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 608)\n",
      "\n",
      "0: 352x640 (no detections), 56.7ms\n",
      "Speed: 1.0ms preprocess, 56.7ms inference, 0.3ms postprocess per image at shape (1, 3, 352, 640)\n",
      "[    0.77248]\n",
      "[]\n",
      "[]\n",
      "\n",
      "0: 640x480 1 person, 75.7ms\n",
      "Speed: 1.8ms preprocess, 75.7ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x608 (no detections), 91.7ms\n",
      "Speed: 1.6ms preprocess, 91.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 608)\n",
      "\n",
      "0: 640x128 (no detections), 27.0ms\n",
      "Speed: 0.5ms preprocess, 27.0ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 128)\n",
      "\n",
      "0: 352x640 (no detections), 56.2ms\n",
      "Speed: 1.1ms preprocess, 56.2ms inference, 0.3ms postprocess per image at shape (1, 3, 352, 640)\n",
      "[    0.66078]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "\n",
      "0: 640x96 (no detections), 22.0ms\n",
      "Speed: 0.5ms preprocess, 22.0ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 96)\n",
      "\n",
      "0: 640x480 (no detections), 73.5ms\n",
      "Speed: 1.5ms preprocess, 73.5ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x640 (no detections), 103.7ms\n",
      "Speed: 1.6ms preprocess, 103.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 352x640 (no detections), 57.1ms\n",
      "Speed: 1.0ms preprocess, 57.1ms inference, 0.4ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 640x128 (no detections), 26.8ms\n",
      "Speed: 0.5ms preprocess, 26.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 128)\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "\n",
      "0: 640x480 1 person, 76.8ms\n",
      "Speed: 1.7ms preprocess, 76.8ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 352x640 (no detections), 55.7ms\n",
      "Speed: 1.0ms preprocess, 55.7ms inference, 0.3ms postprocess per image at shape (1, 3, 352, 640)\n",
      "[     0.6973]\n",
      "[]\n",
      "\n",
      "0: 640x480 1 person, 73.3ms\n",
      "Speed: 1.3ms preprocess, 73.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 480x640 (no detections), 80.9ms\n",
      "Speed: 1.3ms preprocess, 80.9ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x128 (no detections), 26.4ms\n",
      "Speed: 0.5ms preprocess, 26.4ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 128)\n",
      "[    0.67268]\n",
      "[]\n",
      "[]\n",
      "\n",
      "0: 576x640 1 person, 92.6ms\n",
      "Speed: 1.5ms preprocess, 92.6ms inference, 0.5ms postprocess per image at shape (1, 3, 576, 640)\n",
      "\n",
      "0: 480x640 (no detections), 74.1ms\n",
      "Speed: 1.2ms preprocess, 74.1ms inference, 0.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x128 (no detections), 27.1ms\n",
      "Speed: 0.4ms preprocess, 27.1ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 128)\n",
      "\n",
      "0: 640x128 (no detections), 26.7ms\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[113], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m video_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSamsungGear360.mp4\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mprocess_video\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvideo_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43myolo11\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moverlap_threshold\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.0005\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marea_threshold\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m700\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mearly_stop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1000\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[112], line 35\u001B[0m, in \u001B[0;36mprocess_video\u001B[0;34m(video_path, people_detector, scale, overlap_threshold, area_threshold, early_stop)\u001B[0m\n\u001B[1;32m     27\u001B[0m cropped_frames \u001B[38;5;241m=\u001B[39m crop_bboxes(frame, bboxes\u001B[38;5;241m=\u001B[39m[md[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m md \u001B[38;5;129;01min\u001B[39;00m merged_detections])\n\u001B[1;32m     29\u001B[0m \u001B[38;5;66;03m# detect people on cropped frames\u001B[39;00m\n\u001B[1;32m     30\u001B[0m \n\u001B[1;32m     31\u001B[0m \u001B[38;5;66;03m# probs, bboxes, result = people_detector.detect(frame)\u001B[39;00m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;66;03m# annotated_frame = result.plot()\u001B[39;00m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;66;03m# view(annotated_frame)\u001B[39;00m\n\u001B[0;32m---> 35\u001B[0m detections \u001B[38;5;241m=\u001B[39m \u001B[43mpeople_detector\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdetect_on_frames\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcropped_frames\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m detection \u001B[38;5;129;01min\u001B[39;00m detections:\n\u001B[1;32m     37\u001B[0m     probs, _, result \u001B[38;5;241m=\u001B[39m detection\n",
      "Cell \u001B[0;32mIn[103], line 45\u001B[0m, in \u001B[0;36mPeopleDetector.detect_on_frames\u001B[0;34m(self, frames)\u001B[0m\n\u001B[1;32m     43\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mdetect_on_frames\u001B[39m(\u001B[38;5;28mself\u001B[39m, frames: \u001B[38;5;28mlist\u001B[39m[np\u001B[38;5;241m.\u001B[39mndarray]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mtuple\u001B[39m[np\u001B[38;5;241m.\u001B[39marray, np\u001B[38;5;241m.\u001B[39marray, Any]]:\n\u001B[1;32m     44\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\n\u001B[0;32m---> 45\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdetect\u001B[49m\u001B[43m(\u001B[49m\u001B[43mframe\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m frame \u001B[38;5;129;01min\u001B[39;00m frames\n\u001B[1;32m     46\u001B[0m     ]\n",
      "Cell \u001B[0;32mIn[103], line 27\u001B[0m, in \u001B[0;36mPeopleDetector.detect\u001B[0;34m(self, frame)\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mdetect\u001B[39m(\u001B[38;5;28mself\u001B[39m, frame: np\u001B[38;5;241m.\u001B[39mndarray) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mtuple\u001B[39m[np\u001B[38;5;241m.\u001B[39marray, np\u001B[38;5;241m.\u001B[39marray, Any]:\n\u001B[1;32m     23\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;124;03m    :param frame: frame in which detect people\u001B[39;00m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;124;03m    :return: a np.array of the confidences scores, a np.array of the bounding box coordinates, and the result obj\u001B[39;00m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 27\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclasses\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfocus_on_classes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mthreshold\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;66;03m# list of 1 Results object, because we can predict in batches (for video only)\u001B[39;00m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;124;03m    cls: tensor([0., 0., 0., 0., 0., 0.], device='cuda:0')\u001B[39;00m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;124;03m    conf: tensor([0.9429, 0.9262, 0.8841, 0.8833, 0.8824, 0.8773], device='cuda:0')\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     38\u001B[0m \u001B[38;5;124;03m    ])\u001B[39;00m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m     40\u001B[0m     \u001B[38;5;66;03m# print(results)\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/ComputerVision/.venv/lib/python3.12/site-packages/ultralytics/engine/model.py:180\u001B[0m, in \u001B[0;36mModel.__call__\u001B[0;34m(self, source, stream, **kwargs)\u001B[0m\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__call__\u001B[39m(\n\u001B[1;32m    152\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    153\u001B[0m     source: Union[\u001B[38;5;28mstr\u001B[39m, Path, \u001B[38;5;28mint\u001B[39m, Image\u001B[38;5;241m.\u001B[39mImage, \u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m, np\u001B[38;5;241m.\u001B[39mndarray, torch\u001B[38;5;241m.\u001B[39mTensor] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    154\u001B[0m     stream: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    155\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    156\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mlist\u001B[39m:\n\u001B[1;32m    157\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    158\u001B[0m \u001B[38;5;124;03m    Alias for the predict method, enabling the model instance to be callable for predictions.\u001B[39;00m\n\u001B[1;32m    159\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    178\u001B[0m \u001B[38;5;124;03m        ...     print(f\"Detected {len(r)} objects in image\")\u001B[39;00m\n\u001B[1;32m    179\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 180\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43msource\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/ComputerVision/.venv/lib/python3.12/site-packages/ultralytics/engine/model.py:558\u001B[0m, in \u001B[0;36mModel.predict\u001B[0;34m(self, source, stream, predictor, **kwargs)\u001B[0m\n\u001B[1;32m    556\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m prompts \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredictor, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mset_prompts\u001B[39m\u001B[38;5;124m\"\u001B[39m):  \u001B[38;5;66;03m# for SAM-type models\u001B[39;00m\n\u001B[1;32m    557\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredictor\u001B[38;5;241m.\u001B[39mset_prompts(prompts)\n\u001B[0;32m--> 558\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredictor\u001B[38;5;241m.\u001B[39mpredict_cli(source\u001B[38;5;241m=\u001B[39msource) \u001B[38;5;28;01mif\u001B[39;00m is_cli \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredictor\u001B[49m\u001B[43m(\u001B[49m\u001B[43msource\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msource\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/ComputerVision/.venv/lib/python3.12/site-packages/ultralytics/engine/predictor.py:173\u001B[0m, in \u001B[0;36mBasePredictor.__call__\u001B[0;34m(self, source, model, stream, *args, **kwargs)\u001B[0m\n\u001B[1;32m    171\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstream_inference(source, model, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    172\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 173\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstream_inference\u001B[49m\u001B[43m(\u001B[49m\u001B[43msource\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/ComputerVision/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:36\u001B[0m, in \u001B[0;36m_wrap_generator.<locals>.generator_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     34\u001B[0m     \u001B[38;5;66;03m# Issuing `None` to a generator fires it up\u001B[39;00m\n\u001B[1;32m     35\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m---> 36\u001B[0m         response \u001B[38;5;241m=\u001B[39m \u001B[43mgen\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     38\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m     39\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     40\u001B[0m             \u001B[38;5;66;03m# Forward the response to our caller and get its next request\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/ComputerVision/.venv/lib/python3.12/site-packages/ultralytics/engine/predictor.py:283\u001B[0m, in \u001B[0;36mBasePredictor.stream_inference\u001B[0;34m(self, source, model, *args, **kwargs)\u001B[0m\n\u001B[1;32m    281\u001B[0m \u001B[38;5;66;03m# Print batch results\u001B[39;00m\n\u001B[1;32m    282\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mverbose:\n\u001B[0;32m--> 283\u001B[0m     \u001B[43mLOGGER\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minfo\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    285\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrun_callbacks(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mon_predict_batch_end\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    286\u001B[0m \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresults\n",
      "File \u001B[0;32m~/.local/share/uv/python/cpython-3.12.8-linux-x86_64-gnu/lib/python3.12/logging/__init__.py:1539\u001B[0m, in \u001B[0;36mLogger.info\u001B[0;34m(self, msg, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;124;03mLog 'msg % args' with severity 'INFO'.\u001B[39;00m\n\u001B[1;32m   1532\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;124;03mlogger.info(\"Houston, we have a %s\", \"notable problem\", exc_info=True)\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39misEnabledFor(INFO):\n\u001B[0;32m-> 1539\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_log\u001B[49m\u001B[43m(\u001B[49m\u001B[43mINFO\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmsg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/share/uv/python/cpython-3.12.8-linux-x86_64-gnu/lib/python3.12/logging/__init__.py:1684\u001B[0m, in \u001B[0;36mLogger._log\u001B[0;34m(self, level, msg, args, exc_info, extra, stack_info, stacklevel)\u001B[0m\n\u001B[1;32m   1681\u001B[0m         exc_info \u001B[38;5;241m=\u001B[39m sys\u001B[38;5;241m.\u001B[39mexc_info()\n\u001B[1;32m   1682\u001B[0m record \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmakeRecord(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname, level, fn, lno, msg, args,\n\u001B[1;32m   1683\u001B[0m                          exc_info, func, extra, sinfo)\n\u001B[0;32m-> 1684\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrecord\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/share/uv/python/cpython-3.12.8-linux-x86_64-gnu/lib/python3.12/logging/__init__.py:1700\u001B[0m, in \u001B[0;36mLogger.handle\u001B[0;34m(self, record)\u001B[0m\n\u001B[1;32m   1698\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(maybe_record, LogRecord):\n\u001B[1;32m   1699\u001B[0m     record \u001B[38;5;241m=\u001B[39m maybe_record\n\u001B[0;32m-> 1700\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcallHandlers\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrecord\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/share/uv/python/cpython-3.12.8-linux-x86_64-gnu/lib/python3.12/logging/__init__.py:1762\u001B[0m, in \u001B[0;36mLogger.callHandlers\u001B[0;34m(self, record)\u001B[0m\n\u001B[1;32m   1760\u001B[0m     found \u001B[38;5;241m=\u001B[39m found \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   1761\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m record\u001B[38;5;241m.\u001B[39mlevelno \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m hdlr\u001B[38;5;241m.\u001B[39mlevel:\n\u001B[0;32m-> 1762\u001B[0m         \u001B[43mhdlr\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrecord\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1763\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m c\u001B[38;5;241m.\u001B[39mpropagate:\n\u001B[1;32m   1764\u001B[0m     c \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m    \u001B[38;5;66;03m#break out\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/share/uv/python/cpython-3.12.8-linux-x86_64-gnu/lib/python3.12/logging/__init__.py:1028\u001B[0m, in \u001B[0;36mHandler.handle\u001B[0;34m(self, record)\u001B[0m\n\u001B[1;32m   1026\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39macquire()\n\u001B[1;32m   1027\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1028\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43memit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrecord\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1029\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m   1030\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrelease()\n",
      "File \u001B[0;32m~/.local/share/uv/python/cpython-3.12.8-linux-x86_64-gnu/lib/python3.12/logging/__init__.py:1164\u001B[0m, in \u001B[0;36mStreamHandler.emit\u001B[0;34m(self, record)\u001B[0m\n\u001B[1;32m   1162\u001B[0m     \u001B[38;5;66;03m# issue 35046: merged two stream.writes into one.\u001B[39;00m\n\u001B[1;32m   1163\u001B[0m     stream\u001B[38;5;241m.\u001B[39mwrite(msg \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mterminator)\n\u001B[0;32m-> 1164\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mflush\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1165\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mRecursionError\u001B[39;00m:  \u001B[38;5;66;03m# See issue 36272\u001B[39;00m\n\u001B[1;32m   1166\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/share/uv/python/cpython-3.12.8-linux-x86_64-gnu/lib/python3.12/logging/__init__.py:1144\u001B[0m, in \u001B[0;36mStreamHandler.flush\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1142\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1143\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstream \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstream, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mflush\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m-> 1144\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mflush\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1145\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m   1146\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrelease()\n",
      "File \u001B[0;32m~/PycharmProjects/ComputerVision/.venv/lib/python3.12/site-packages/ipykernel/iostream.py:609\u001B[0m, in \u001B[0;36mOutStream.flush\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    607\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpub_thread\u001B[38;5;241m.\u001B[39mschedule(evt\u001B[38;5;241m.\u001B[39mset)\n\u001B[1;32m    608\u001B[0m     \u001B[38;5;66;03m# and give a timeout to avoid\u001B[39;00m\n\u001B[0;32m--> 609\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[43mevt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mflush_timeout\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m    610\u001B[0m         \u001B[38;5;66;03m# write directly to __stderr__ instead of warning because\u001B[39;00m\n\u001B[1;32m    611\u001B[0m         \u001B[38;5;66;03m# if this is happening sys.stderr may be the problem.\u001B[39;00m\n\u001B[1;32m    612\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIOStream.flush timed out\u001B[39m\u001B[38;5;124m\"\u001B[39m, file\u001B[38;5;241m=\u001B[39msys\u001B[38;5;241m.\u001B[39m__stderr__)\n\u001B[1;32m    613\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/.local/share/uv/python/cpython-3.12.8-linux-x86_64-gnu/lib/python3.12/threading.py:655\u001B[0m, in \u001B[0;36mEvent.wait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    653\u001B[0m signaled \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_flag\n\u001B[1;32m    654\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m signaled:\n\u001B[0;32m--> 655\u001B[0m     signaled \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_cond\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    656\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m signaled\n",
      "File \u001B[0;32m~/.local/share/uv/python/cpython-3.12.8-linux-x86_64-gnu/lib/python3.12/threading.py:359\u001B[0m, in \u001B[0;36mCondition.wait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    357\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    358\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m--> 359\u001B[0m         gotit \u001B[38;5;241m=\u001B[39m \u001B[43mwaiter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43macquire\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    360\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    361\u001B[0m         gotit \u001B[38;5;241m=\u001B[39m waiter\u001B[38;5;241m.\u001B[39macquire(\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 113
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# People Detection TEST\n",
   "id": "e027d6955f3367c9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Define the base functions for people detection. <br/>\n",
    "It Takes as input an array of frames containing the motion detected by MOG2 above"
   ],
   "id": "f1de7d0bb118f7b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# imports\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image"
   ],
   "id": "91ab0819e0ce2630",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load a model\n",
    "model = PeopleDetector(\"./yolo11x.pt\")  # load an official model, or use local path"
   ],
   "id": "4b39a6e73355cefc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def test_yolo11(video_path: str, scale: float = 0.5):\n",
    "    # Open the video file\n",
    "    cap = cv.VideoCapture(video_path)\n",
    "\n",
    "    # Loop through the video frames\n",
    "    while cap.isOpened():\n",
    "        # Read a frame from the video\n",
    "        success, frame = cap.read()\n",
    "\n",
    "        if success:\n",
    "            # Run YOLO inference on the frame\n",
    "            # frame = cv.resize(frame, None, fx=0.5, fy=0.5, interpolation=cv.INTER_LINEAR)\n",
    "            results = model(frame)\n",
    "\n",
    "            # Visualize the results on the frame\n",
    "            annotated_frame = results[0].plot()\n",
    "            annotated_frame = cv.resize(annotated_frame, None, fx=scale, fy=scale, interpolation=cv.INTER_LINEAR)\n",
    "            # Display the annotated frame\n",
    "            cv.imshow(\"YOLO Inference\", annotated_frame)\n",
    "\n",
    "            # Break the loop if 'q' is pressed\n",
    "            if cv.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "                break\n",
    "        else:\n",
    "            # Break the loop if the end of the video is reached\n",
    "            break\n",
    "\n",
    "    # Release the video capture object and close the display window\n",
    "    cap.release()\n",
    "    cv.destroyAllWindows()"
   ],
   "id": "7ca67ec510bc568a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Main test YOLO11",
   "id": "a311a48f20a0b0cb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "test_yolo11(\"SamsungGear360.mp4\", scale=0.25)",
   "id": "10ce9b273702bcbf",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
