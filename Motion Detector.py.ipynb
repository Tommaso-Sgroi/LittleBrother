{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "from motion_detector.motion_detector import MotionDetector\n",
    "from people_detector.people_detector import PeopleDetector\n",
    "from utils.bbox_utils import crop_bboxes\n",
    "from utils.view import view"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2447958e18206ed",
   "metadata": {},
   "source": [
    "## PROCESS VIDEO\n",
    "*Here happens the magick*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a3902c9f9ae4861",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(video_path: str, motion_detector: MotionDetector, people_detector: PeopleDetector, scale: float = 0.5, early_stop=None, starts_from=0):\n",
    "    \"\"\"\n",
    "    Processes the input video.\n",
    "    Parameters:\n",
    "        video_path: Path to the video file.\n",
    "        scale: Scaling factor for resizing frames.\n",
    "        overlap_threshold: Threshold for merging overlapping detections using IoU.\n",
    "        area_threshold: Minimum area for detected bounding boxes.\n",
    "        people_detector: YOLO model to detect people.\n",
    "        early_stop: stop after n frames\n",
    "    \"\"\"\n",
    "    capture = cv.VideoCapture(video_path)\n",
    "    iterations = 0\n",
    "    while early_stop is None or iterations < early_stop:\n",
    "        iterations += 1\n",
    "        ret, frame = capture.read()\n",
    "        if iterations <= starts_from:\n",
    "            print(\"skipping frame {}\".format(iterations))\n",
    "            continue\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # frame = cv.resize(frame, None, fx=scale, fy=scale, interpolation=cv.INTER_LINEAR)\n",
    "        frame = cv.resize(frame, None, fx=scale, fy=scale, interpolation=cv.INTER_AREA)\n",
    "\n",
    "        # detections, merged_detections, frame = mog2_movement_detection(frame, background_subtractor=back_sub, area_threshold=area_threshold,\n",
    "        #                         overlap_threshold=overlap_threshold, draw=False)\n",
    "\n",
    "        detections, merged_detections, frame = motion_detector(frame, draw=False)\n",
    "\n",
    "        if not merged_detections:\n",
    "            continue\n",
    "\n",
    "        # drop area colum\n",
    "        # cropped_frames = crop_bboxes(frame, bboxes=[md[:-1] for md in merged_detections])\n",
    "        merged_detections = np.array(merged_detections)\n",
    "        cropped_frames = crop_bboxes(frame, bboxes=merged_detections[:, :-1]) # a-la numpy\n",
    "\n",
    "        # detect people on cropped frames\n",
    "\n",
    "        # probs, bboxes, result = people_detector.detect(frame)\n",
    "        # annotated_frame = result.plot()\n",
    "        # view(annotated_frame)\n",
    "\n",
    "        detections = people_detector.detect_on_frames(cropped_frames)\n",
    "        for detection in detections:\n",
    "            probs, _, result = detection\n",
    "            print(probs)\n",
    "            if len(probs) == 0:\n",
    "                continue\n",
    "            annotated_frame = result.plot()\n",
    "            # Image.fromarray(annotated_frame).save('./runs/detect/{}.png'.format(iterations))\n",
    "\n",
    "            view(annotated_frame)\n",
    "\n",
    "        # for frame in cropped_frames:\n",
    "        #     stop = view(frame, scale=scale)\n",
    "        #     if not stop:\n",
    "        #         break\n",
    "    else:\n",
    "        print(f\"Stopped after {iterations} frames due early stopping condition.\")\n",
    "\n",
    "    capture.release()\n",
    "    cv.destroyAllWindows()\n",
    "\n",
    "    if sys.platform == 'darwin':\n",
    "        for _ in range(4):\n",
    "            cv.waitKey(1)\n",
    "\n",
    "    cv.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d04d81258a7f91",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c51cdddc4972c528",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolosize = 'x'\n",
    "yolo11 = PeopleDetector(f\"yolo11{yolosize}.pt\", verbose=False,)\n",
    "yolo11.to('cpu')\n",
    "\n",
    "\n",
    "overlap_threshold=0.0005\n",
    "area_threshold=700\n",
    "motion_detector = MotionDetector(area_threshold=area_threshold, overlap_threshold=overlap_threshold)\n",
    "\n",
    "\n",
    "# probs, bboxes, _ = yolo11.detect(\"The-Office-HD-Background.jpg\")\n",
    "# print('confidence scores', probs)\n",
    "# print('bboxes', bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3946034d83bc6843",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = 'SamsungGear360.mp4'\n",
    "process_video(video_path, motion_detector, yolo11, scale=0.5) # , starts_from=3000, early_stop=3500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e027d6955f3367c9",
   "metadata": {},
   "source": [
    "# People Detection TEST\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1de7d0bb118f7b1",
   "metadata": {},
   "source": [
    "Define the base functions for people detection. <br/>\n",
    "It Takes as input an array of frames containing the motion detected by MOG2 above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ab0819e0ce2630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b39a6e73355cefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a model\n",
    "model = PeopleDetector(\"./yolo11x.pt\")  # load an official model, or use local path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca67ec510bc568a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_yolo11(video_path: str, scale: float = 0.5):\n",
    "    # Open the video file\n",
    "    cap = cv.VideoCapture(video_path)\n",
    "\n",
    "    # Loop through the video frames\n",
    "    while cap.isOpened():\n",
    "        # Read a frame from the video\n",
    "        success, frame = cap.read()\n",
    "\n",
    "        if success:\n",
    "            # Run YOLO inference on the frame\n",
    "            # frame = cv.resize(frame, None, fx=0.5, fy=0.5, interpolation=cv.INTER_LINEAR)\n",
    "            results = model(frame)\n",
    "\n",
    "            # Visualize the results on the frame\n",
    "            annotated_frame = results[0].plot()\n",
    "            annotated_frame = cv.resize(annotated_frame, None, fx=scale, fy=scale, interpolation=cv.INTER_LINEAR)\n",
    "            # Display the annotated frame\n",
    "            cv.imshow(\"YOLO Inference\", annotated_frame)\n",
    "\n",
    "            # Break the loop if 'q' is pressed\n",
    "            if cv.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "                break\n",
    "        else:\n",
    "            # Break the loop if the end of the video is reached\n",
    "            break\n",
    "\n",
    "    # Release the video capture object and close the display window\n",
    "    cap.release()\n",
    "    cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a311a48f20a0b0cb",
   "metadata": {},
   "source": [
    "## Main test YOLO11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ce9b273702bcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_yolo11(\"SamsungGear360.mp4\", scale=0.25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
