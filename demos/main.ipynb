{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92d1a7dc8826403",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from logging import CRITICAL, DEBUG, INFO\n",
    "from time import sleep\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "import utils.logger as logger\n",
    "from camera.video_frame_initializer import initializer\n",
    "from face_recognizer.face_recognizer import FaceRecognizer\n",
    "from motion_detector.motion_detector import MotionDetector, MotionDetectorMap\n",
    "from people_detector.people_detector import PeopleDetector\n",
    "from utils.bbox_utils import crop_bboxes\n",
    "from utils.view import view\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2447958e18206ed",
   "metadata": {},
   "source": [
    "## PROCESS VIDEO\n",
    "*Here happens the magick*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3902c9f9ae4861",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(videos: list[str], motion_detector_map: MotionDetectorMap, people_detector: PeopleDetector,\n",
    "                  face_recognizer: FaceRecognizer, scale: float = 0.5,\n",
    "                  early_stop=None, starts_from=0):\n",
    "    \"\"\"\n",
    "    Processes the input video.\n",
    "    Parameters:\n",
    "        videos_path: Paths to the video files.\n",
    "        scale: Scaling factor for resizing frames.\n",
    "        overlap_threshold: Threshold for merging overlapping detections using IoU.\n",
    "        area_threshold: Minimum area for detected bounding boxes.\n",
    "        people_detector: YOLO model to detect people.\n",
    "        early_stop: stop after n frames\n",
    "    \"\"\"\n",
    "    controller = initializer(videos, timeout=0.1,)\n",
    "    controller.start_frame_sources()\n",
    "    # controller.start() # avoid multithread :(\n",
    "    sleep(1)\n",
    "    print( controller.has_empty_buffer(), controller.has_alive_sources())\n",
    "    while not controller.has_empty_buffer() or controller.has_alive_sources():\n",
    "        frames = controller.fetch_and_get_frames()\n",
    "\n",
    "        if len(frames) == 0:\n",
    "            continue\n",
    "\n",
    "        for video_id, frame in frames:\n",
    "            # frame = cv.resize(frame, None, fx=scale, fy=scale, interpolation=cv.INTER_LINEAR)\n",
    "            frame = cv.resize(frame, None, fx=scale, fy=scale, interpolation=cv.INTER_AREA)\n",
    "            \n",
    "            motion_detector = motion_detector_map.get_motion_detector(video_id)\n",
    "            detections, merged_detections, frame = motion_detector(frame, draw=False)\n",
    "\n",
    "            if not merged_detections:\n",
    "                continue\n",
    "\n",
    "            merged_detections = np.array(merged_detections)\n",
    "            cropped_frames = crop_bboxes(frame, bboxes=merged_detections[:, :-1])  # a-la numpy\n",
    "\n",
    "            detections = people_detector.detect_on_frames(cropped_frames)\n",
    "            for detection in detections:\n",
    "                probs, _, result = detection\n",
    "                if len(probs) == 0:\n",
    "                    continue\n",
    "\n",
    "                detected_person_image = result.orig_img\n",
    "\n",
    "                # Face detection + recognition\n",
    "\n",
    "                # Skip too small images\n",
    "                if detected_person_image.shape[0] < face_recognizer.min_face_size or detected_person_image.shape[\n",
    "                    1] < face_recognizer.min_face_size:\n",
    "                    continue\n",
    "    \n",
    "                faces = face_recognizer.recognize_faces(detected_person_image)\n",
    "                annotated_frame = result.plot()\n",
    "                view(annotated_frame, winname=video_id)\n",
    "                if len(faces) > 0:\n",
    "                    print(f\"Frame {video_id}: {len(faces)} faces detected.\")\n",
    "                                     \n",
    "                    for detected_face in faces:\n",
    "                        if detected_face['label'] is not None:\n",
    "                            print(\n",
    "                                f\"Detected face: {detected_face['label']} with confidence {detected_face['confidence']}\")\n",
    "                        else:\n",
    "                            print(f\"Detected unrecognized face! ðŸ˜­\")\n",
    "    else:\n",
    "        print('No resources available')\n",
    "    \n",
    "    cv.destroyAllWindows()\n",
    "\n",
    "    if sys.platform == 'darwin':\n",
    "        for _ in range(4):\n",
    "            cv.waitKey(1)\n",
    "\n",
    "    cv.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d04d81258a7f91",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51cdddc4972c528",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolosize = 'n'\n",
    "yolo11 = PeopleDetector(f\"yolo11{yolosize}.pt\", verbose=False, )\n",
    "yolo11.to('cpu')\n",
    "\n",
    "overlap_threshold = 0.0005\n",
    "area_threshold = 700\n",
    "motion_detector = MotionDetectorMap(area_threshold=area_threshold, overlap_threshold=overlap_threshold)\n",
    "\n",
    "face_recognizer = FaceRecognizer(threshold=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3946034d83bc6843",
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = \\\n",
    "        \"\"\"./datasets/WiseNET/wisenet_dataset/video_sets/set_1/video1_1.avi\n",
    "        ./datasets/WiseNET/wisenet_dataset/video_sets/set_1/video1_2.avi\n",
    "        ./datasets/WiseNET/wisenet_dataset/video_sets/set_1/video1_3.avi\n",
    "        ./datasets/WiseNET/wisenet_dataset/video_sets/set_1/video1_4.avi\n",
    "        ./datasets/WiseNET/wisenet_dataset/video_sets/set_1/video1_5.avi\"\"\".split('\\n')\n",
    "\n",
    "videos_path = [video.strip() for video in videos]\n",
    "\n",
    "logger.init_logger(DEBUG)\n",
    "process_video(videos_path, motion_detector, yolo11, face_recognizer=face_recognizer,\n",
    "              scale=1)  # , starts_from=3000, early_stop=3500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e027d6955f3367c9",
   "metadata": {},
   "source": [
    "# People Detection TEST\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1de7d0bb118f7b1",
   "metadata": {},
   "source": [
    "Define the base functions for people detection. <br/>\n",
    "It Takes as input an array of frames containing the motion detected by MOG2 above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b39a6e73355cefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a model\n",
    "model = PeopleDetector(\"./yolo11x.pt\")  # load an official model, or use local path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca67ec510bc568a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_yolo11(video_path: str, scale: float = 0.5):\n",
    "    # Open the video file\n",
    "    cap = cv.VideoCapture(video_path)\n",
    "\n",
    "    # Loop through the video frames\n",
    "    while cap.isOpened():\n",
    "        # Read a frame from the video\n",
    "        success, frame = cap.read()\n",
    "\n",
    "        if success:\n",
    "            # Run YOLO inference on the frame\n",
    "            # frame = cv.resize(frame, None, fx=0.5, fy=0.5, interpolation=cv.INTER_LINEAR)\n",
    "            results = model(frame)\n",
    "\n",
    "            # Visualize the results on the frame\n",
    "            annotated_frame = results[0].plot()\n",
    "            annotated_frame = cv.resize(annotated_frame, None, fx=scale, fy=scale, interpolation=cv.INTER_LINEAR)\n",
    "            # Display the annotated frame\n",
    "            cv.imshow(\"YOLO Inference\", annotated_frame)\n",
    "\n",
    "            # Break the loop if 'q' is pressed\n",
    "            if cv.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "                break\n",
    "        else:\n",
    "            # Break the loop if the end of the video is reached\n",
    "            break\n",
    "\n",
    "    # Release the video capture object and close the display window\n",
    "    cap.release()\n",
    "    cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a311a48f20a0b0cb",
   "metadata": {},
   "source": [
    "## Main test YOLO11"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
